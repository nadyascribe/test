# Project requirements
## local infra
* [docker](https://docs.docker.com/engine/install/) (checked with 23.0.6)
* [docker-compose](https://docs.docker.com/compose/install/linux/#install-the-plugin-manually) (checked with 2.17.3)
## go-related
* [go](https://go.dev/doc/install) 1.20.1
* [golangci-lint](https://golangci-lint.run/usage/install/#local-installation) 1.51.1
* [sqlc](https://github.com/kyleconroy/sqlc) 1.16.0
## go env setup
1. `go env -w GOPRIVATE="github.com/scribe-security/*"` - downloads all required dependencies.
2. `git config --global url."git@github.com:scribe-security".insteadOf "https://github.com/scribe-security"`
## py-related
* python 3.10.10. It's recommended not to change main host's python version, but use virtual environment instead. As a suggestion you can use [pyenv](https://github.com/pyenv/pyenv).
* [airflow](https://airflow.apache.org/docs/apache-airflow/stable/index.html) 2.5.3
## py env setup
1. Create virtual environment. Must be python 3.10.10.
2. `pip install -r requirements.txt` - installs all required dependencies.
3. `pip install -r requirements-dev.txt` - installs all required dev dependencies (pytest, formatters, linters etc.)

# Local startup
1. `cp airflow_conns.env airflow_conns.env_local` - copy example connections file to the local one.     
2. `make build-scribe-cmd-debug` - this command builds the current repo's golang-written CLI-tool.    
3. `make init-airflow-local` - starts local airflow with all required dependencies.
4. (optional) if you want to use local minio you have to run `make persist-data-local` to download all attestations (only sbom for now, see the command into a makefile) which are stored in your local database to the local filesystem. All subsequent `make init-airflow-local` will import stored data into minio automatically, so no need to invoke `make persist-data-local` more. It requires to set aws credentials for "aws_default" connection. 
5. `make stop-airflow-local` - stops local airflow.

## Local development notes
All binary or 3rd party tools must be stored within `dags/tools` storage with `.bin` extension (`dats/tools/*.bin` added to `.gitignore`).    
All python dependencies must be stored within `requirements.txt` file.  


## S3 objects local persistence
`make persist-data-local` - downloads all attestations which are stored in your local database to the local filesystem. All subsequent `make init-airflow-local` will import stored data into minio automatically, so no need to invoke `make persist-data-local` more. Command requirements:
* set aws credentials for "aws_default" connection.
* present `etl/.env_local` (won't be commited as far as it's defined into `.gitignore`) and change `POSTGRES_HOST` to "localhost" and hostname within `AIRFLOW__DATABASE__SQL_ALCHEMY_CONN` to `localhost`. Take into account that `Makefile` includes all variables presented there during all `make` targets invocation.   

## Running migrations
* `make migration-up` - runs all migrations.  
* `make migration-down` - rolls back the last migration.  
* `make name="foo bar baz" migration-new` - creates new empty migration with specified name. If name is not specified, it will be empty.  
* `make name="foo bar baz" migration-new-auto` - creates new autogenerated migration with specified name. If name is not specified, it will be empty.
* after any schema changes `make generate-sqlc` must be invoked to regenerate sqlc-related files.  

## Running linters and formatters
* `lint-py` - runs python linters and formatters
* `lint-go` - runs go linters and formatters
* `lint-all` - runs all linters and formatters

## AWS
`./.dev/aws.sh` contains several functions that may help you to handle aws-related stuff.  
Obviously, you have to have aws cli installed and configured and also you have to have access to several required resources (ec2-describe-instances, ecs-list-tasks, ssm-get-parameters and so on).   
### postgres port forward
```bash
source ./.dev/aws.sh
# environment localPort
postgresPortForward dev 6432
```

### airflow ui port forward
```bash
source ./.dev/aws.sh
# environment localPort
airflowUIPortForward dev 5000
```
### interactive console inside of container
```bash
source ./.dev/aws.sh
# environment componentName
interactiveEcsExec dev scheduler
```

## Help for github cli
`gh workflow --repo OWNER/REPO_NAME run --ref antonio-antuan-patch-1 build.yml` - runs workflow `build.yml` for specified `repo` and `ref` (e.g. branch).
`gh run list --repo OWNER/REPO_NAME --workflow=build.yml` - shows workflow runs
`gh run list --repo OWNER/REPO_NAME --workflow=build.yml --json startedAt,createdAt,updatedAt,headBranch,name,workflowDatabaseId,status,url |jq '.[0]'` - shows the last workflow run
`gh run view --repo OWNER/REPO_NAME $(gh run list --repo OWNER/REPO_NAME --workflow=deployment.yml --json databaseId |jq '.[0].databaseId')` - view status of the last workflow run
